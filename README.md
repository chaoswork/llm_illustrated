# llm_illustrated

本电子书目前进度完成不到 10%，可以关注下方公众号回复 "看图学大模型" 来获取最新版。

![看图学大模型](./images/kantuxue_qr.jpeg)


节选一些文章的图片，尽量用清晰易懂的方式来讲述大模型相关技术。

比如 

# self attention 的结构和代码：

![self-attention](./images/self-attention-1.png)

# 绝对位置编码的解释

![pe](./images/sinusoidal_pe.png)

# KV Cache 图解

![kv-cache](./images/kv_cache_diff.jpeg)

# transformers 的组成

![transformers](./images/transformers_compose.png)

# 达特茅斯会议参与人员关系图谱

![dart](./images/ai_graph.jpeg)



目前 PDF 排版等还略有问题，Latex 还需要略微调整。

